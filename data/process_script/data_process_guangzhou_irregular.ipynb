{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import scipy\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import utils_BayTIDE as utils\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214, 61, 144)\n",
      "(214, 8784)\n",
      "(214, 500)\n"
     ]
    }
   ],
   "source": [
    "# impoet tensor.mat using scipy\n",
    "tensor = loadmat('/data/fang/BayTIDE_data/raw/guangzhou/Data/tensor.mat')\n",
    "tensor = tensor['tensor']\n",
    "print(tensor.shape)\n",
    "\n",
    "# only use the first 5 days, resahpe to 2D\n",
    "# sub_tensor = tensor[:,:5,:] \n",
    "sub_tensor = tensor.reshape(214,61*144)\n",
    "print(sub_tensor.shape)\n",
    "\n",
    "# to match the TIDER, just use the first 500 time points\n",
    "sub_tensor = sub_tensor[:,-501:-1]\n",
    "print(sub_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "73.195\n",
      "1.484\n",
      "37.43098849065421\n"
     ]
    }
   ],
   "source": [
    "# check the zero ratio\n",
    "print(np.sum(sub_tensor==0)/sub_tensor.size)\n",
    "\n",
    "# print max, min and mean\n",
    "print(np.max(sub_tensor))\n",
    "print(np.min(sub_tensor))\n",
    "print(np.mean(sub_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_save = {}\n",
    "data_save['ndims'] = sub_tensor.shape\n",
    "\n",
    "data_save['raw_data'] = sub_tensor\n",
    "\n",
    "data_save['data'] = []\n",
    "\n",
    "# sorted unique timestamps, set as (normalized to 0-1 ) regular time intervals at current stage\n",
    "data_save['time_uni'] = np.linspace(0,1,sub_tensor.shape[1])\n",
    "\n",
    "def generate_random_mask( shape, drop_rate=0.2, valid_rate=0.1):\n",
    "    \"\"\"\n",
    "    train_ratio: 1-valid_rate-drop_rate\n",
    "    test_ratio: drop_rate\n",
    "    valid_ratio: valid_rate\n",
    "    \"\"\"\n",
    "    N,T = shape\n",
    "\n",
    "    train_T = int(T*(1-valid_rate-drop_rate))\n",
    "\n",
    "    T_index = np.arange(T)\n",
    "\n",
    "    np.random.shuffle(T_index)\n",
    "\n",
    "    mask_train = np.zeros((N,T))\n",
    "    mask_test = np.zeros((N,T))\n",
    "    mask_valid = np.zeros((N,T))\n",
    "\n",
    "    mask_train[:,T_index[:train_T]] = 1\n",
    "    mask_test[:,T_index[train_T:train_T+int(T*drop_rate)]] = 1\n",
    "    mask_valid[:,T_index[train_T+int(T*drop_rate):]] = 1\n",
    "\n",
    "    # mask_train_list = []\n",
    "    # mask_test_list = []\n",
    "    # mask_valid_list = []\n",
    "\n",
    "    # make block-wise missingness\n",
    "\n",
    "\n",
    "    # for t in range(T):\n",
    "\n",
    "    #     mask = np.random.rand(N)\n",
    "    #     mask_train = np.where(mask>drop_rate+valid_rate, 1, 0)\n",
    "    #     mask_test = np.where(mask<drop_rate, 1, 0)\n",
    "    #     mask_valid = np.where((mask>drop_rate) & (mask<drop_rate+valid_rate), 1, 0)\n",
    "\n",
    "    #     mask_train_list.append(mask_train)\n",
    "    #     mask_test_list.append(mask_test)\n",
    "    #     mask_valid_list.append(mask_valid)\n",
    "    \n",
    "    # mask_train = np.stack(mask_train_list, axis=1)\n",
    "    # mask_test = np.stack(mask_test_list, axis=1)\n",
    "    # mask_valid = np.stack(mask_valid_list, axis=1)\n",
    "\n",
    "    return mask_train, mask_test, mask_valid\n",
    "\n",
    "fold = 2\n",
    "drop_rate = 0.2\n",
    "valid_rate = 0.1\n",
    "\n",
    "for i in range(fold):\n",
    "    mask_train, mask_test, mask_valid = generate_random_mask(sub_tensor.shape, drop_rate, valid_rate)\n",
    "    data_save['data'].append({'mask_train':mask_train, 'mask_test':mask_test, 'mask_valid':mask_valid})\n",
    "\n",
    "file_name = '/data/fang/BayTIDE_data/guangzhou_impute'+'_r_%.1f'%(drop_rate)+'_irregulate.npy'\n",
    "np.save(file_name, data_save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20118691588785045"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_test.sum()/mask_train.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/fang/BayTIDE_data/guangzhou_impute_r_0.2_irregulate.npy'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
